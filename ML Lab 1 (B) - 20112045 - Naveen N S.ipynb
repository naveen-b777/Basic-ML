{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 1 :- Getting Familiar with Machine Learning Libraries (III)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**Submitted by:-** <br>\n",
    "\n",
    "Name: **Naveen N S** <br>\n",
    "Register Number: **20112045** <br>\n",
    "Class: **4BSCDS** <br> \n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Overview\n",
    "<hr/>\n",
    "\n",
    "## Objectives :\n",
    "\n",
    " - **Exploring various sub-parts of sklearn library**\n",
    "      - train_test_split from sklearn.model_selection\n",
    "      - make_classification and load_iris from sklearn.datasets\n",
    "      - make_regression and load_boston from sklearn.datasets\n",
    "      - DummyClassifier and DummyRegressor from sklearn.dummy\n",
    "      - accuracy_score, classification_report, and confusion_matrix from sklearn.metrics <br>\n",
    "     \n",
    "      \n",
    " - **Answering below mentioned questions**\n",
    "      - What are the different parameters of the above functions/methods that are part of the above SKLearn modules?\n",
    "      - What is the effect, when you modify certain parameters that are present in the same?\n",
    "      - How to get different train and test datasets?\n",
    "      - Identify which are features and which are targets in Part 1b, and Part 1c (make_classification and make_regression  would depend on your inputs on the function call)\n",
    "      - Identify what the things mentioned in Part 1d stands for. \n",
    "      - Making use of Part 1d, explore the various options available under Part 1e.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "<hr/>\n",
    "\n",
    "1] Lab Overview <br>\n",
    "2] References<br>\n",
    "3] train_test_split<br>\n",
    "4] make_classification<br>\n",
    "5] load_iris<br>\n",
    "6] make_regression<br>\n",
    "7] load_boston<br>\n",
    "8] DummyClassifier<br>\n",
    "9] DummyRegressor<br>\n",
    "10] accuracy_score<br>\n",
    "11] classification_report<br>\n",
    "12] confusion_matrix<br>\n",
    "13] What are the different parameters of the above functions/methods that are part of the above SKLearn modules?<br>\n",
    "14] What is the effect, when you modify certain parameters that are present in the same?<br>\n",
    "15] How to get different train and test datasets?<br>\n",
    "16] Identify which are features and which are targets in Part 1b, and Part 1c (make_classification and make_regression would depend on your inputs on the function call)<br>\n",
    "17] Identify what the things mentioned in Part 1d stands for. <br>\n",
    "18] Making use of Part 1d, explore the various options available under Part 1e.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<hr/>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/classes.html\n",
    "- https://www.datacamp.com/blog/scikit-learn-cheat-sheet-python-machine-learning\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html#stratification\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
    "- https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html\n",
    "- https://www.geeksforgeeks.org/ml-dummy-classifiers-using-sklearn/\n",
    "- https://www.geeksforgeeks.org/ml-dummy-classifiers-using-sklearn/\n",
    "- https://www.geeksforgeeks.org/dummy-regressor/\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "- https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "<hr/>\n",
    "\n",
    "#### **[A]** Explore subparts of *train_test_split* from sklearn.model_selection of sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "<hr/>\n",
    "\n",
    " - I imported a Wine_Quality dataset from kaggle inorder to explore this command \n",
    " - Importing the subpart of the library\n",
    " - Getting information about the command\n",
    " - Exploring the command and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1138            6.3             0.510         0.13             2.3      0.076   \n",
       "1139            6.8             0.620         0.08             1.9      0.068   \n",
       "1140            6.2             0.600         0.08             2.0      0.090   \n",
       "1141            5.9             0.550         0.10             2.2      0.062   \n",
       "1142            5.9             0.645         0.12             2.0      0.075   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "\n",
       "      alcohol  quality    Id  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     1  \n",
       "2         9.8        5     2  \n",
       "3         9.8        6     3  \n",
       "4         9.4        5     4  \n",
       "...       ...      ...   ...  \n",
       "1138     11.0        6  1592  \n",
       "1139      9.5        6  1593  \n",
       "1140     10.5        5  1594  \n",
       "1141     11.2        6  1595  \n",
       "1142     10.2        5  1597  \n",
       "\n",
       "[1143 rows x 13 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('wineQT.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sk.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** TRAINING DATASET ************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.071</td>\n",
       "      <td>23.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.99633</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.074</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99580</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.048</td>\n",
       "      <td>13.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.99374</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.60</td>\n",
       "      <td>12.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>9.1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.097</td>\n",
       "      <td>9.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99976</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.039</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99344</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>9.2</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.097</td>\n",
       "      <td>29.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99880</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>9.4</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.075</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.073</td>\n",
       "      <td>46.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>9.3</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.085</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.99708</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.062</td>\n",
       "      <td>31.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99728</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.65</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "1086            6.1              0.32         0.25             2.3      0.071   \n",
       "393             6.8              0.51         0.01             2.1      0.074   \n",
       "780             6.3              0.57         0.28             2.1      0.048   \n",
       "587             9.1              0.66         0.15             3.2      0.097   \n",
       "1064            7.1              0.22         0.49             1.8      0.039   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "323             9.2              0.63         0.21             2.7      0.097   \n",
       "180             9.4              0.34         0.37             2.2      0.075   \n",
       "450             7.9              0.35         0.21             1.9      0.073   \n",
       "607             9.3              0.43         0.44             1.9      0.085   \n",
       "1043            7.3              0.48         0.32             2.1      0.062   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "1086                 23.0                  58.0  0.99633  3.42       0.97   \n",
       "393                   9.0                  25.0  0.99580  3.33       0.56   \n",
       "780                  13.0                  49.0  0.99374  3.41       0.60   \n",
       "587                   9.0                  59.0  0.99976  3.28       0.54   \n",
       "1064                  8.0                  18.0  0.99344  3.39       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "323                  29.0                  65.0  0.99880  3.28       0.58   \n",
       "180                   5.0                  13.0  0.99800  3.22       0.62   \n",
       "450                  46.0                 102.0  0.99640  3.27       0.58   \n",
       "607                   9.0                  22.0  0.99708  3.28       0.55   \n",
       "1043                 31.0                  54.0  0.99728  3.30       0.65   \n",
       "\n",
       "      alcohol  quality    Id  \n",
       "1086     10.6        5  1522  \n",
       "393       9.5        6   550  \n",
       "780      12.8        5  1105  \n",
       "587       9.6        5   819  \n",
       "1064     12.4        6  1490  \n",
       "...       ...      ...   ...  \n",
       "323       9.6        5   457  \n",
       "180       9.2        5   256  \n",
       "450       9.5        5   634  \n",
       "607       9.5        5   851  \n",
       "1043     10.0        7  1466  \n",
       "\n",
       "[857 rows x 13 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here by default, unless specified, split will be 25% test-data and 75% training-data\n",
    "\n",
    "x_train,x_test = sk.train_test_split(data)\n",
    "\n",
    "print(\"************************** TRAINING DATASET ************************\")\n",
    "print()\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** TESTING DATASET *************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.079</td>\n",
       "      <td>19.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.99697</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>8.4</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.081</td>\n",
       "      <td>32.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.72</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>12.6</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99870</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.8</td>\n",
       "      <td>8</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.070</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99575</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.077</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.99625</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.084</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>10.7</td>\n",
       "      <td>6</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.057</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.99488</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.084</td>\n",
       "      <td>13.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.99538</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>10.6</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.080</td>\n",
       "      <td>21.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.99850</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.57</td>\n",
       "      <td>10.1</td>\n",
       "      <td>5</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "699            7.7             0.600         0.06             2.0      0.079   \n",
       "649            8.4             0.360         0.32             2.2      0.081   \n",
       "310           12.6             0.310         0.72             2.2      0.072   \n",
       "186            8.0             0.520         0.03             1.7      0.070   \n",
       "184            7.9             0.330         0.23             1.7      0.077   \n",
       "..             ...               ...          ...             ...        ...   \n",
       "381            8.1             0.825         0.24             2.1      0.084   \n",
       "540            6.3             0.980         0.01             2.0      0.057   \n",
       "955            7.5             0.510         0.02             1.7      0.084   \n",
       "506           10.6             1.025         0.43             2.8      0.080   \n",
       "15             7.9             0.430         0.21             1.6      0.106   \n",
       "\n",
       "     free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "699                 19.0                  41.0  0.99697  3.39       0.62   \n",
       "649                 32.0                  79.0  0.99640  3.30       0.72   \n",
       "310                  6.0                  29.0  0.99870  2.88       0.82   \n",
       "186                 10.0                  35.0  0.99575  3.34       0.57   \n",
       "184                 18.0                  45.0  0.99625  3.29       0.65   \n",
       "..                   ...                   ...      ...   ...        ...   \n",
       "381                  5.0                  13.0  0.99720  3.37       0.77   \n",
       "540                 15.0                  33.0  0.99488  3.60       0.46   \n",
       "955                 13.0                  31.0  0.99538  3.36       0.54   \n",
       "506                 21.0                  84.0  0.99850  3.06       0.57   \n",
       "15                  10.0                  37.0  0.99660  3.17       0.91   \n",
       "\n",
       "     alcohol  quality    Id  \n",
       "699     10.1        6   995  \n",
       "649     11.0        6   918  \n",
       "310      9.8        8   440  \n",
       "186     10.0        5   262  \n",
       "184      9.3        5   260  \n",
       "..       ...      ...   ...  \n",
       "381     10.7        6   537  \n",
       "540     11.2        6   756  \n",
       "955     10.5        6  1343  \n",
       "506     10.1        5   710  \n",
       "15       9.5        5    22  \n",
       "\n",
       "[286 rows x 13 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"************************** TESTING DATASET *************************\")\n",
    "print()\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** TRAINING DATASET ************************\n",
      "\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "1130            7.4             0.350         0.33             2.4      0.068   \n",
      "453             9.5             0.885         0.27             2.3      0.084   \n",
      "1072            6.2             0.440         0.39             2.5      0.077   \n",
      "877             6.5             0.670         0.00             4.3      0.057   \n",
      "193             7.9             0.545         0.06             4.0      0.087   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1099            6.2             0.520         0.08             4.4      0.071   \n",
      "466            10.7             0.430         0.39             2.2      0.106   \n",
      "299             9.5             0.780         0.22             1.9      0.077   \n",
      "493             5.1             0.470         0.02             1.3      0.034   \n",
      "527             9.0             0.690         0.00             2.4      0.088   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "1130                  9.0                  26.0  0.99470  3.36       0.60   \n",
      "453                  31.0                 145.0  0.99780  3.24       0.53   \n",
      "1072                  6.0                  14.0  0.99555  3.51       0.69   \n",
      "877                  11.0                  20.0  0.99488  3.45       0.56   \n",
      "193                  27.0                  61.0  0.99650  3.36       0.67   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1099                 11.0                  32.0  0.99646  3.56       0.63   \n",
      "466                   8.0                  32.0  0.99860  2.89       0.50   \n",
      "299                   6.0                  32.0  0.99880  3.26       0.56   \n",
      "493                  18.0                  44.0  0.99210  3.90       0.62   \n",
      "527                  19.0                  38.0  0.99900  3.35       0.60   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "1130     11.9        6  1580  \n",
      "453       9.4        5   637  \n",
      "1072     11.0        6  1503  \n",
      "877      11.8        4  1239  \n",
      "193      10.7        6   270  \n",
      "...       ...      ...   ...  \n",
      "1099     11.6        6  1540  \n",
      "466       9.6        5   656  \n",
      "299      10.6        6   427  \n",
      "493      12.8        6   695  \n",
      "527       9.3        5   739  \n",
      "\n",
      "[914 rows x 13 columns]\n",
      "\n",
      "************************** TESTING DATASET *************************\n",
      "\n",
      "     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "835            9.8             0.390         0.43            1.65      0.068   \n",
      "226            7.4             0.360         0.29            2.60      0.087   \n",
      "199           11.4             0.260         0.44            3.60      0.071   \n",
      "158            6.8             0.610         0.04            1.50      0.057   \n",
      "597            6.7             0.280         0.28            2.40      0.012   \n",
      "..             ...               ...          ...             ...        ...   \n",
      "86             7.8             0.500         0.17            1.60      0.082   \n",
      "164            8.5             0.370         0.20            2.80      0.090   \n",
      "99             8.1             0.670         0.55            1.80      0.117   \n",
      "286            7.1             0.735         0.16            1.90      0.100   \n",
      "947            7.2             0.835         0.00            2.00      0.166   \n",
      "\n",
      "     free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "835                  5.0                  11.0  0.99478  3.19       0.46   \n",
      "226                 26.0                  72.0  0.99645  3.39       0.68   \n",
      "199                  6.0                  19.0  0.99860  3.12       0.82   \n",
      "158                  5.0                  10.0  0.99525  3.42       0.60   \n",
      "597                 36.0                 100.0  0.99064  3.26       0.39   \n",
      "..                   ...                   ...      ...   ...        ...   \n",
      "86                  21.0                 102.0  0.99600  3.39       0.48   \n",
      "164                 18.0                  58.0  0.99800  3.34       0.70   \n",
      "99                  32.0                 141.0  0.99680  3.17       0.62   \n",
      "286                 15.0                  77.0  0.99660  3.27       0.64   \n",
      "947                  4.0                  11.0  0.99608  3.39       0.52   \n",
      "\n",
      "     alcohol  quality    Id  \n",
      "835     11.4        5  1181  \n",
      "226     11.0        5   314  \n",
      "199      9.3        6   280  \n",
      "158      9.5        5   222  \n",
      "597     11.7        7   836  \n",
      "..       ...      ...   ...  \n",
      "86       9.5        5   124  \n",
      "164      9.6        6   232  \n",
      "99       9.4        5   145  \n",
      "286      9.3        5   412  \n",
      "947     10.0        5  1334  \n",
      "\n",
      "[229 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# using some other parameters like test_size,train_size,random_state,shuffle,stratify\n",
    "\n",
    "# setting 20% training data and 80% testing data\n",
    "y_train,y_test = sk.train_test_split(data,test_size=0.2,train_size=0.8,random_state=2,shuffle=True)\n",
    "\n",
    "print(\"************************** TRAINING DATASET ************************\")\n",
    "print()\n",
    "print(y_train)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"************************** TESTING DATASET *************************\")\n",
    "print()\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "<hr/>\n",
    "\n",
    "#### [B] Explore *make_classification* and *load_iris* from sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "<hr/>\n",
    "\n",
    " - Importing required subparts of sklearn\n",
    " - Getting information about the command using '?' \n",
    " - Creating a random classification problem using the command\n",
    " - Loading iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skd.make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** THE GENERATED SAMPLE ***************************\n",
      "\n",
      "[[ 4.05813079e-01  5.81126289e-01  3.00349819e-01 -8.36965103e-01\n",
      "  -2.20251153e-02]\n",
      " [ 6.80024104e-01  1.38482896e+00  1.11368120e+00 -1.13750666e+00\n",
      "  -4.66046593e-01]\n",
      " [ 7.86328029e-03 -9.31031513e-01 -1.39348678e+00 -6.23736940e-01\n",
      "   7.90211266e-01]\n",
      " [-3.65547535e-02 -1.16392089e+00 -1.67774654e+00 -6.41268156e-01\n",
      "   8.50464703e-01]\n",
      " [-8.89722365e-01 -2.17857532e+00 -2.00166879e+00  1.25185221e+00\n",
      "   1.72756670e+00]\n",
      " [-5.41604132e-01 -4.42338629e-01  9.40128840e-02  1.33187498e+00\n",
      "  -1.36275092e-02]\n",
      " [-1.06943543e-02  1.45546487e+00  2.17619716e+00  9.70305130e-01\n",
      "   5.53468585e-02]\n",
      " [-1.27985088e+00 -2.08713178e+00 -1.32499538e+00  2.47561089e+00\n",
      "  -9.30303747e-01]\n",
      " [ 6.93226981e-01  1.50288765e+00  1.27069412e+00 -1.10081093e+00\n",
      "   5.43921150e-01]\n",
      " [-4.03934489e-01 -9.85605382e-01 -9.03607174e-01  5.70578289e-01\n",
      "   5.00572525e-02]\n",
      " [ 7.19941742e-01  5.68213892e-01 -1.54337353e-01 -1.78318079e+00\n",
      "   1.10726865e-01]\n",
      " [ 6.89608168e-01  7.64499942e-01  1.79202987e-01 -1.56606355e+00\n",
      "   6.23900444e-01]\n",
      " [ 2.59828801e-01  2.43497738e-01  1.36482732e-03 -6.18779026e-01\n",
      "   3.45050086e-02]\n",
      " [-1.91069291e-01  3.50221426e-01  7.84981073e-01  7.96270261e-01\n",
      "   1.04600797e+00]\n",
      " [-1.27158278e+00 -1.99581625e+00 -1.20085451e+00  2.50979834e+00\n",
      "   8.88980112e-01]\n",
      " [-1.71419786e-01  1.75050508e-01  4.97609315e-01  6.24665803e-01\n",
      "   1.69546183e+00]\n",
      " [ 1.79458641e-01 -7.88582319e-01 -1.41985135e+00 -1.04422684e+00\n",
      "   1.07790748e+00]\n",
      " [ 2.06057091e-01  3.60695810e-01  2.49954194e-01 -3.82672813e-01\n",
      "  -7.23721365e-01]\n",
      " [ 9.21147555e-01  2.07878649e+00  1.80991372e+00 -1.41001447e+00\n",
      "  -1.10468650e+00]\n",
      " [-8.57559773e-01 -4.14025233e-01  5.74103540e-01  2.29347487e+00\n",
      "  -2.76192007e-01]]\n",
      "\n",
      "*********************** THE INTEGER LABEL FOR CLASS MEMBERSHIP OF EACH SAMPLE ***************************\n",
      "\n",
      "[1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a,b=skd.make_classification(n_samples=20,n_features=5,n_informative=2,n_redundant=2,n_repeated=0,n_classes=2,\n",
    "                            n_clusters_per_class=2,weights=None,flip_y=0.01,class_sep=1.0,hypercube=True,shift=0.0,\n",
    "                            scale=1.0,shuffle=True,random_state=None)\n",
    "\n",
    "print(\"*********************** THE GENERATED SAMPLE ***************************\")\n",
    "print()\n",
    "print(a)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"*********************** THE INTEGER LABEL FOR CLASS MEMBERSHIP OF EACH SAMPLE ***************************\")\n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skd.load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\Users\\\\avgee\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing iris_dataset as 'sklearn.utils.Bunch' type\n",
    "\n",
    "iris_data = skd.load_iris()\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing it as pandas dataframe\n",
    "\n",
    "df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "#### [C] Explore *make_regression* and *load_boston* from sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "<hr/>\n",
    "\n",
    " - Importing sklearn.datasets library\n",
    " - Learning about different parameters of *make_regression* command\n",
    " - Creating a random regression problem using *make_regression* command \n",
    " \n",
    "<hr/>\n",
    " - Loading Boston dataset from sklearn.datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skd.make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** INPUT SAMPLES ***********************\n",
      "\n",
      "[[ 4.52820796e-01 -1.27256373e-01  1.65797453e+00 -1.63452129e+00\n",
      "  -6.53701983e-01  1.37526452e+00  2.17670686e-01  1.42060848e+00\n",
      "   1.28141221e+00 -1.02766393e-02 -5.15730733e-01 -2.97603302e-01\n",
      "   9.18040287e-01 -2.56034595e-01  1.24693061e+00  1.87860323e-01\n",
      "  -3.68432913e-01 -5.57274885e-01 -1.28673963e+00  4.32433666e-01]\n",
      " [ 2.39405857e-01 -7.53841428e-01  1.03955858e-05 -8.04933727e-01\n",
      "  -2.03254007e+00  9.07474996e-01 -5.59614293e-01 -3.34321250e-01\n",
      "  -9.01044169e-01 -1.10277324e+00  3.66767864e-01  1.80190374e+00\n",
      "   2.61313895e-01 -7.62297844e-01  2.89323479e-01 -1.41971239e-02\n",
      "   1.97096025e-01 -3.87453984e-01  5.91523141e-01 -1.05585180e+00]\n",
      " [ 9.69148178e-01  1.17618439e+00 -9.47877520e-01 -4.53738553e-01\n",
      "   1.79832820e+00  6.71658182e-01 -1.18932571e+00  1.52114204e+00\n",
      "   1.58111458e-01  6.26939615e-01  5.97379222e-01  8.05223092e-01\n",
      "   8.12947686e-01  5.44319278e-01  8.82663468e-01  1.03647276e+00\n",
      "   5.20923167e-01  6.11045670e-01  4.16592958e-01  1.04290371e+00]\n",
      " [-1.25129555e+00  7.83988566e-01 -1.47532499e+00 -1.18691754e+00\n",
      "   6.77923785e-02 -2.90807701e-01 -1.07868102e+00 -1.03379078e+00\n",
      "   4.95452879e-01 -1.43758658e+00  8.04505098e-01 -1.67466007e+00\n",
      "   7.62452611e-01  4.05150606e-01 -7.22672405e-01 -7.20780931e-01\n",
      "  -2.36760877e+00  1.78548921e-01 -6.71237437e-01 -8.39663135e-01]\n",
      " [ 7.54006703e-01 -1.16605540e+00  2.67011204e-01  4.30820743e-01\n",
      "  -9.05724035e-01  3.70205880e-02  2.04824707e-02 -5.99902925e-01\n",
      "  -7.23987828e-01 -9.78166445e-01 -3.16617888e-01 -1.07500623e+00\n",
      "  -1.87238606e+00 -8.01785969e-01  2.00014027e+00  5.06460113e-01\n",
      "  -7.63864077e-01  8.74400486e-01  1.07504882e+00 -1.07107676e+00]\n",
      " [ 4.31040851e-01  1.36176578e+00  3.13078497e-01  9.66177147e-01\n",
      "   7.74862796e-01 -8.11488544e-01  8.89634036e-01 -4.49499252e-01\n",
      "  -1.31995575e+00 -3.65681000e-02 -6.20586274e-01 -1.17082676e+00\n",
      "  -1.24184898e+00 -5.31366565e-01  1.86875673e+00  2.37409722e-01\n",
      "  -1.31442876e+00  4.10723764e-01  2.32129058e+00 -1.08226054e+00]\n",
      " [ 2.29386412e-01 -8.25130941e-01 -1.14510340e+00 -6.28339361e-02\n",
      "   1.16972862e+00  1.44159259e+00 -8.84209652e-01  1.59638065e-01\n",
      "  -5.29370471e-01  6.63762488e-01  6.33603821e-01 -6.89683429e-01\n",
      "   2.74345322e-02  8.97479193e-01  1.61862019e-01  1.54941712e+00\n",
      "   1.47834196e+00 -3.28066579e-01  1.19677330e+00 -1.86424683e-01]\n",
      " [-4.45615973e-01  1.76671424e+00 -1.03391323e-01 -1.31624113e+00\n",
      "   2.25742321e+00  1.32863551e+00 -3.47781271e-01 -1.14557512e+00\n",
      "  -1.60560135e+00 -8.21765756e-02 -3.64289449e-01 -2.88636908e-01\n",
      "  -1.34418930e+00  3.36534235e-01  6.92329937e-01  7.94055087e-01\n",
      "  -2.00225930e-01  2.82917952e-01 -1.99425904e-01 -1.89976401e+00]\n",
      " [-1.42639726e+00 -5.21957165e-02 -3.49754102e-02 -1.70542820e-01\n",
      "   1.96207776e+00 -5.30025915e-01 -1.05748566e+00 -1.06035551e+00\n",
      "  -1.94971646e+00  8.14051775e-01 -2.38411518e-01  2.79499904e-01\n",
      "   1.93460204e+00 -5.20146285e-01  4.67238250e-02 -5.95088194e-01\n",
      "  -1.30253700e+00 -9.08488209e-01 -1.19075805e+00 -1.58089981e-01]\n",
      " [ 5.07113390e-01 -1.79081260e+00  1.65034754e+00  5.96215986e-01\n",
      "   1.86108658e+00  4.84154249e-01 -1.81966768e+00  6.14775383e-01\n",
      "   3.67019066e-01  3.96397882e-01  2.28328668e+00  3.93118862e-01\n",
      "  -3.97872904e-02  8.47730078e-01  1.01104108e+00  6.28289893e-02\n",
      "   1.39109855e+00  6.49287714e-01 -1.97240184e-01 -2.14842898e+00]\n",
      " [ 1.47927421e+00 -8.94468275e-01  2.12690235e+00 -7.37723309e-01\n",
      "   7.37919909e-02  3.83160119e-02 -6.28859823e-01 -3.24062082e-01\n",
      "  -1.53038473e+00  2.08206012e-02  9.87417514e-01  2.39682671e-01\n",
      "  -2.82608141e-01 -4.79297329e-01 -6.95610510e-01 -1.02558951e+00\n",
      "  -2.44014570e-01  1.75344273e-01 -9.87394198e-02 -6.73464396e-01]\n",
      " [-3.05216345e-01 -1.79254722e-02 -1.17002837e+00 -8.47614041e-01\n",
      "  -6.46835732e-01 -1.39461530e+00  1.49014495e+00  7.82441647e-01\n",
      "  -8.16674755e-01 -7.83279673e-01 -5.41503320e-01 -5.03011203e-01\n",
      "  -1.15828751e+00 -2.17349361e+00  1.09381785e+00 -4.62139092e-01\n",
      "   1.76050296e-01  1.04395119e+00  9.23557558e-01 -9.96443362e-01]\n",
      " [-1.82796640e-01 -5.17150443e-02 -7.91797440e-01  6.71761864e-01\n",
      "   1.09583445e+00 -6.01840320e-01  7.39340815e-01  7.42211592e-01\n",
      "  -3.57613977e-01  6.40956321e-01 -1.50023462e+00 -1.37423856e+00\n",
      "   1.14896078e+00 -4.18087283e-01  3.20805836e-03 -8.97058474e-01\n",
      "   1.94896156e-01  7.97845468e-01  4.31990321e-01  7.72451885e-02]\n",
      " [-8.59396534e-01 -5.05183767e-01  4.42173108e-01 -1.55003064e+00\n",
      "  -1.96287457e+00 -7.01483677e-02 -1.24104540e+00  2.13019687e-01\n",
      "   1.56045334e+00 -2.53954355e-01 -2.89119959e-03 -2.69448506e+00\n",
      "   3.43641882e-01 -9.40008757e-01 -1.05575743e-01 -9.62367277e-01\n",
      "   9.54694375e-01 -5.34426402e-01 -4.37159868e-01 -4.05407453e-01]\n",
      " [-1.98764709e-01 -4.09468007e-01  5.13946245e-01 -2.93721040e-01\n",
      "  -1.05485521e+00  1.26604121e+00  1.02505228e+00  4.29045002e-01\n",
      "  -6.41009161e-01 -9.14617927e-02 -1.64871326e+00  3.54757876e-01\n",
      "  -1.75256041e-01  1.11690450e-01  7.18607719e-01 -9.57701508e-01\n",
      "   2.38935706e-01 -1.00968041e+00 -9.52575987e-01 -4.17302728e-02]\n",
      " [ 8.96793669e-01  1.79352570e-01 -1.48775137e+00  6.53467552e-01\n",
      "  -3.40462989e-02 -8.08113794e-01 -2.51743321e-01  1.08884843e+00\n",
      "   1.04066115e+00  1.21471110e+00  6.40684284e-01  1.59316521e-03\n",
      "  -9.03413473e-02 -3.70364397e-01 -5.03236100e-01  3.91578510e-01\n",
      "   1.93055476e+00 -1.65013411e+00 -4.08370682e-01  2.05471840e-02]\n",
      " [-7.23737639e-01 -2.94775035e-01 -8.92752675e-01  2.44619611e-01\n",
      "   3.40427890e-01 -5.74000717e-01  3.70192376e-01  9.43232688e-01\n",
      "   6.17910070e-02  1.42248468e+00 -9.18083097e-01 -1.60883312e+00\n",
      "   5.07785202e-01 -2.18180099e+00  1.34317908e+00  1.06709778e+00\n",
      "   4.55018654e-01  1.91796684e+00 -6.06551172e-01  1.86328358e+00]\n",
      " [ 1.47077291e+00  1.20229415e+00  9.08989438e-01 -2.02364760e+00\n",
      "  -7.49195393e-01 -1.64716478e+00  1.60613474e+00 -6.94379837e-01\n",
      "  -3.67761109e-01  7.84861147e-01 -1.09107262e+00 -7.88976228e-01\n",
      "   2.18945674e+00 -6.21958642e-01  3.56432975e-02 -8.76869921e-03\n",
      "  -1.00026975e-01 -4.26539773e-01 -4.15398075e-01 -5.91632578e-01]\n",
      " [ 1.30770552e+00  1.17448563e+00 -1.01245895e+00 -5.91885363e-02\n",
      "   7.54652274e-01  1.49169092e+00  3.21002331e-01 -5.02138354e-01\n",
      "  -7.71177088e-01  1.33332720e+00 -1.76077643e+00  9.48456427e-01\n",
      "   4.05996075e-01  1.12495043e-01 -1.22332900e-01  2.10614653e-01\n",
      "  -4.22023872e-01  3.48123079e-01 -1.21049258e+00 -4.40596477e-01]\n",
      " [-2.70311995e-03  1.39211124e+00  7.88858646e-01  5.61918497e-01\n",
      "  -2.40658412e-01  7.21988898e-02  6.18938220e-01  1.51776719e-01\n",
      "  -1.52598525e+00 -2.63024759e+00 -1.65160636e+00  1.75880568e+00\n",
      "   2.15753115e+00 -4.24070256e-01  1.06961159e+00 -6.53083929e-01\n",
      "  -1.41234983e+00 -1.56764625e+00  1.60447048e-02  1.37094850e+00]]\n",
      "\n",
      "**************** OUTPUT SAMPLES **********************\n",
      "\n",
      "[ 386.25820193  -39.94553202  250.13793732 -107.1278241   -11.25087194\n",
      " -126.97768876  112.80596448 -100.70098569 -321.72434115  161.7967911\n",
      " -231.39716337  -75.99240864  -43.39433166  127.12630473   58.53366332\n",
      "  121.67635625  133.45967413 -198.30297701  -12.83108745 -102.80041866]\n"
     ]
    }
   ],
   "source": [
    "# creating a regression model using make_regression command of sklearn.datasets\n",
    "x,y = skd.make_regression(n_samples=20,n_features=20,n_informative=5,n_targets=1,bias=0.0,effective_rank=None,\n",
    "    tail_strength=0.5,noise=0.0,shuffle=True,coef=False,random_state=None)\n",
    "\n",
    "print(\"**************** INPUT SAMPLES ***********************\")\n",
    "print()\n",
    "print(x)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"**************** OUTPUT SAMPLES **********************\")\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skd.load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\avgee\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading boston data set in sklearn.utils.Bunch type\n",
    "\n",
    "boston_data = skd.load_boston()\n",
    "boston_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing it as pandas dataframe\n",
    "\n",
    "df = pd.DataFrame(data=boston_data.data, columns=boston_data.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "<hr/>\n",
    "\n",
    "#### [D] *DummyClassifier* and *DummyRegressor* from sklearn.dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "<hr/>\n",
    "\n",
    "- Import required library\n",
    "- I will be using Wine Quality Dataset\n",
    "- Here, 'Quality' is the target variable and rest all except 'Id' are features\n",
    "- Using these arrays to fit and predict values\n",
    "- Trying out different parameter values to figure-out best classifier method ('most frequent' is best in this case as it got highest score)\n",
    "- Comparing results using SeaBorn Library\n",
    "<hr/>\n",
    "\n",
    "- Similar approach is followed for DummyRegressor.\n",
    "- I have used Iris dataset \n",
    "- Matplot Library has been used to compare different parameters based on mean_squared_error, r2_score, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "df = pd.read_csv('wineQT.csv')\n",
    "\n",
    "# separating dependent and independent variables\n",
    "X = df.drop('Id',axis=1)\n",
    "X = X.drop('quality',axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "#splitting data into training and testind data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using different \n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "strategies = ['most_frequent', 'stratified', 'uniform', 'constant']\n",
    "\n",
    "test_scores = []\n",
    "for s in strategies:\n",
    "    if s =='constant':\n",
    "        dclf = DummyClassifier(strategy = s, random_state = 0, constant =6)\n",
    "    else:\n",
    "        dclf = DummyClassifier(strategy = s, random_state = 0)\n",
    "    dclf.fit(X_train, y_train)\n",
    "    score = dclf.score(X_test, y_test)\n",
    "    test_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avgee\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-77b8eb39bf12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstripplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxlabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'Strategy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'Test Score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'show'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.stripplot(strategies, test_scores);\n",
    "ax.set(xlabel ='Strategy', ylabel ='Test Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data[:, None, 1]\n",
    "y= iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Mean squared error **********************\n",
      "0.5877416756176155\n",
      "\n",
      "******************** Median absolute error **********************\n",
      "1.0\n",
      "\n",
      "******************** r2_score (Dummy Mean) **********************\n",
      "-0.03499875559980081\n",
      "\n",
      "******************** r2_score (Dummy Median) **********************\n",
      "-0.019512195121951237\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n",
    "lm_dummy_median = DummyRegressor(strategy = 'median').fit(X_train, y_train)\n",
    "\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n",
    "y_predict_dummy_median = lm_dummy_median.predict(X_test)\n",
    "\n",
    "print(\"******************** Mean squared error **********************\")\n",
    "print(mean_squared_error(y_test,y_predict_dummy_mean))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"******************** Median absolute error **********************\")\n",
    "print(median_absolute_error(y_test,y_predict_dummy_median))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"******************** r2_score (Dummy Mean) **********************\")\n",
    "print(r2_score(y_test, y_predict_dummy_mean))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"******************** r2_score (Dummy Median) **********************\")\n",
    "print(r2_score(y_test, y_predict_dummy_median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWyklEQVR4nO3dX4xc5X3G8e/DrFE7BCVNvCEI27NE4qLrCIg9ciGgYLqAzL9akXJhC+UCJVmxgJS0UirIBait9io3iITIWJFFo11AlYgTFGFspKaFBvFnlhowSYwch5iVq3qBNIQQCZn8ejFn4XiY2Tmz82/X7/ORjnbOe953zm+PXz87e87MHkUEZmaWjjOGXYCZmQ2Wg9/MLDEOfjOzxDj4zcwS4+A3M0vMyLALaGbt2rUxNjY27DLMzFaNubm5NyJitEjfFRn8Y2Nj1Gq1YZdhZrZqSPpt0b4+1WNmlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlpi2wS9pvaSfSfqlpFckfaNJH0m6V9IRSS9J2pTbtk3S4WzbHb3+BlI2OzvL2NgYZ5xxBmNjY8zOzg5kvxs3bkTSB8vGjRsLj+225m7GD+t4DWu/Zi1FxJILcC6wKXt8NvAqMN7Q5zpgHyDgEuDZrL0E/Br4LHAm8GLj2GbL5s2bw5Y2MzMT5XI5gA+WcrkcMzMzfd3v+Pj4KftcXMbHx/teczfjh3W8hrVfSw9QizbZurgU6hSnhvxPgKsb2u4HdubWD2c/MC4F9ufa7wTubLcPB397lUqlaQBXKpW+7rfZPheXftfczfhhHa9h7dfS00nwd3SOX9IY8Hng2YZN5wGv59bns7ZW7c2ee1JSTVJtYWGhk7KSdOzYsY7aV4Jua+5m/LCO12r8d7LTX+Hgl/Qx4BHgmxHxduPmJkNiifaPNkbsjohqRFRHRwt96jhpGzZs6Kh9Jei25m7GD+t4rcZ/Jzv9FQp+SWuoh/5sRPyoSZd5YH1ufR1wfIl269L09DTlcvmUtnK5zPT0dF/3Oz4+3lF7Xrc1dzN+WMdrWPs1W1K7c0HUX7X/ELhniT7Xc+rF3eey9hHgKHA+H17c3dhunz7HX8zMzExUKpWQFJVKZWAXDBsv8Ba5sLuo25q7GT+s4zWs/Vpa6OAcv6LNPXclXQ48BbwM/Dlr/jawIfvBsUuSgO8B24B3gZsjopaNvw64h/o7fPZERNuXOtVqNfxH2szMipM0FxHVIn3b/nXOiPgvmp+rz/cJ4LYW2x4DHitSjJmZ9Z8/uWtmlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSWm7Y1YJO0BbgBORMTnmmz/FnBT7vn+GhiNiLckvQb8AXgfOFn07jBmZtY/RV7xP0D9lopNRcR3IuLiiLgYuBP4z4h4K9flymy7Q9/MbAVoG/wR8STwVrt+mZ3AQ11VZGZmfdWzc/ySytR/M3gk1xzAAUlzkibbjJ+UVJNUW1hY6FVZZmbWoJcXd28Eft5wmueyiNgEXAvcJumLrQZHxO6IqEZEdXR0tIdlmZlZXi+DfwcNp3ki4nj29QSwF9jSw/2Zmdky9CT4JX0cuAL4Sa7tLElnLz4GrgEO9WJ/Zma2fEXezvkQsBVYK2keuBtYAxARu7JuXwIORMQfc0PPAfZKWtzPgxHxeO9KNzOz5Wgb/BGxs0CfB6i/7TPfdhS4aLmFmZlZf/iTu2ZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWLaBr+kPZJOSGp620RJWyX9XtLBbLkrt22bpMOSjki6o5eFm5nZ8hR5xf8AsK1Nn6ci4uJs+WcASSXgPuBaYBzYKWm8m2LNzKx7bYM/Ip4E3lrGc28BjkTE0Yh4D3gY2L6M5zEzsx7q1Tn+SyW9KGmfpI1Z23nA67k+81lbU5ImJdUk1RYWFnpUlpmZNepF8L8AVCLiIuC7wI+zdjXpG62eJCJ2R0Q1Iqqjo6M9KMvMzJrpOvgj4u2IeCd7/BiwRtJa6q/w1+e6rgOOd7s/MzPrTtfBL+kzkpQ93pI955vA88AFks6XdCawA3i02/2ZmVl3Rtp1kPQQsBVYK2keuBtYAxARu4AvA1OSTgJ/AnZERAAnJd0O7AdKwJ6IeKUv34WZmRWmekavLNVqNWq12rDLMDNbNSTNRUS1SF9/ctfMLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS0zb4Je2RdELSoRbbb5L0UrY8Lemi3LbXJL0s6aAk/4F9M7MVoMgr/geAbUts/w1wRURcCPwLsLth+5URcXHRGwSYmVl/tb31YkQ8KWlsie1P51afoX5TdTMzW6F6fY7/q8C+3HoAByTNSZpcaqCkSUk1SbWFhYUel2VmZovavuIvStKV1IP/8lzzZRFxXNKngSck/Soinmw2PiJ2k50mqlarK+9GwGZmp4mevOKXdCHwA2B7RLy52B4Rx7OvJ4C9wJZe7M/MzJav6+CXtAH4EfCViHg1136WpLMXHwPXAE3fGWRmZoPT9lSPpIeArcBaSfPA3cAagIjYBdwFfAr4viSAk9k7eM4B9mZtI8CDEfF4H74HMzPrQJF39exss/1rwNeatB8FLvroCDMzGyZ/ctfMLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwS4+A3M0uMg9/MLDEOfjOzxDj4zcwSc3oFv+TFixcv7Ze5Odi9u32/zZvr2bJ5c/u+u3fXn7fI/gEmJ1tv67O2wS9pj6QTkpreNlF190o6IuklSZty27ZJOpxtu6OXhTeanZ3t59ObmZ0+ImLJBfgisAk41GL7dcA+QMAlwLNZewn4NfBZ4EzgRWC83f4igs2bN0cnZmZmolwuB/DBUi6XY2Zmpq9jh6mbuqempk4Zt7hMTU21HTs+Pt507Pj4eNuxExMTTcdOTEwU+p67qTvFOWJpAWpRIF8jon3w15+PsSWC/35gZ279MHAucCmwP9d+J3Bnkf11GvyVSiX7DxlevHjx0nap1SLuv799v02b6hmzaVP7vvffX3/eIvuPiPj611tvW45Ogr8X5/jPA17Prc9nba3am5I0KakmqbawsNBRAceOHeuov5lZynoR/M2uRsQS7U1FxO6IqEZEdXR0tKMCNmzYkCvlw6VSGWv7s7dSGfvIuKJjh7l0U3epNNJ0bKk00nZss3GLSz/Hdlt3N8drtc4RL62XzZvr11bb9ZubqyfL3Fz7vpOT9ectsn+oXwtuta3fehH888D63Po64PgS7T03PT1NuVw+pa1cLjM9Pd3XscPUTd2Tk5MdteeNj4931J43MTHRUXujbupOcY6YtVTkfBBLn+O/nlMv7j6XtY8AR4Hz+fDi7sYi++v0HH9E/QJcpVIJSVGpVDq68NbN2GHqpu6pqakolUoBRKlUKnSBdFHjBd4iF3YXNV7gLXphtxd1pzhHLB10cI5f0eZ3C0kPAVuBtcD/AncDa7IfGrskCfgesA14F7g5ImrZ2OuAe6i/w2dPRBR6iVStVqNWqxXpamZmgKS5iKgW6TvSrkNE7GyzPYDbWmx7DHisSCFmZjYYp9cnd83MrC0Hv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSWmUPBL2ibpsKQjku5osv1bkg5myyFJ70v6ZLbtNUkvZ9t8Wy0zsyFrewcuSSXgPuBq6jdQf17SoxHxi8U+EfEd4DtZ/xuBv4+It3JPc2VEvNHTys3MbFmKvOLfAhyJiKMR8R7wMLB9if47gYd6UZyZmfVekeA/D3g9tz6ftX2EpDL1m64/kmsO4ICkOUmTrXYiaVJSTVJtYWGhQFlmZrYcRYJfTdqiRd8bgZ83nOa5LCI2AdcCt0n6YrOBEbE7IqoRUR0dHS1QlpmZLUeR4J8H1ufW1wHHW/TdQcNpnog4nn09AeylfurIzMyGpEjwPw9cIOl8SWdSD/dHGztJ+jhwBfCTXNtZks5efAxcAxzqReFmZrY8bd/VExEnJd0O7AdKwJ6IeEXSLdn2XVnXLwEHIuKPueHnAHslLe7rwYh4vJffgJmZdUYRrU7XD0+1Wo1azW/5NzMrStJcRFSL9PUnd83MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxDn4zs8Q4+M3MEuPgNzNLjIPfzCwxhYJf0jZJhyUdkXRHk+1bJf1e0sFsuavoWDMzG6y2d+CSVALuA66mfv/d5yU9GhG/aOj6VETcsMyxZmY2IEVe8W8BjkTE0Yh4D3gY2F7w+bsZa2ZmfVAk+M8DXs+tz2dtjS6V9KKkfZI2djgWSZOSapJqCwsLBcoyM7PlKBL8atLWeKPeF4BKRFwEfBf4cQdj640RuyOiGhHV0dHRAmWZmdlyFAn+eWB9bn0dcDzfISLejoh3ssePAWskrS0y1szMBqtI8D8PXCDpfElnAjuAR/MdJH1GkrLHW7LnfbPIWDMzG6y27+qJiJOSbgf2AyVgT0S8IumWbPsu4MvAlKSTwJ+AHRERQNOxffpezMysANXzeWWpVqtRq9WGXYaZ2aohaS4iqkX6+pO7ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgHv5lZYgoFv6Rtkg5LOiLpjibbb5L0UrY8Lemi3LbXJL0s6aAk313FzGzI2t56UVIJuA+4mvrN05+X9GhE/CLX7TfAFRHxO0nXAruBv8ltvzIi3uhh3WZmtkxFXvFvAY5ExNGIeA94GNie7xART0fE77LVZ4B1vS3TzMx6pUjwnwe8nlufz9pa+SqwL7cewAFJc5ImWw2SNCmpJqm2sLBQoCwzM1uOtqd6ADVpa3qHdklXUg/+y3PNl0XEcUmfBp6Q9KuIePIjTxixm/opIqrV6sq7A7yZ2WmiyCv+eWB9bn0dcLyxk6QLgR8A2yPizcX2iDiefT0B7KV+6sjMzIakSPA/D1wg6XxJZwI7gEfzHSRtAH4EfCUiXs21nyXp7MXHwDXAoV4Vb2ZmnWt7qiciTkq6HdgPlIA9EfGKpFuy7buAu4BPAd+XBHAyIqrAOcDerG0EeDAiHu/Ld2JmZoUoYuWdTq9Wq1Gr+S3/ZmZFSZrLXnC35U/umpklxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiSkU/JK2STos6YikO5psl6R7s+0vSdpUdKylZXZ2lrGxMc444wzGxsaYnZ0d6PjV5qqrrkLSB8tVV13V0fhbb72VkZERJDEyMsKtt946kLHd6Ha/q3GODLzmiFhyoX67xV8DnwXOBF4Exhv6XAfsAwRcAjxbdGyzZfPmzWGnn5mZmSiXywF8sJTL5ZiZmRnI+NVmYmLilO91cZmYmCg0fmpqqun4qampvo7tRrf7XY1zpFc1A7Vok62LS5HgvxTYn1u/E7izoc/9wM7c+mHg3CJjmy0O/tNTpVJp+p+6UqkMZPxq0+x7XVyKKJVKTceWSqW+ju1Gt/tdjXOkVzV3EvxFTvWcB7yeW5/P2or0KTIWAEmTkmqSagsLCwXKstXm2LFjHbX3enxq3n///Y7aezW2G93udzXOkWHUXCT41aSt8Q7trfoUGVtvjNgdEdWIqI6OjhYoy1abDRs2dNTe6/GpKZVKHbX3amw3ut3vapwjw6i5SPDPA+tz6+uA4wX7FBlriZienqZcLp/SVi6XmZ6eHsj41WZiYqKj9kaTk5MdtfdqbDe63e9qnCNDqbnduSBgBDgKnM+HF2g3NvS5nlMv7j5XdGyzxef4T18zMzNRqVRCUlQqlY4vYHU7frVpvMBb9MLuoqmpqQ/Om5dKpY4uznYzthvd7nc1zpFe1EwH5/hV7780SdcB91B/l86eiJiWdEv2g2OXJAHfA7YB7wI3R0St1dh2+6tWq1Gr1drWZWZmdZLmIqJaqG+R4B80B7+ZWWc6CX5/ctfMLDEOfjOzxDj4zcwS4+A3M0vMiry4K2kB+G0fnnot8EYfnrdbrqtzK7U219UZ19WZpeqqREShT7+uyODvF0m1ole9B8l1dW6l1ua6OuO6OtOrunyqx8wsMQ5+M7PEpBb8u4ddQAuuq3MrtTbX1RnX1Zme1JXUOX4zM0vvFb+ZWfIc/GZmiTktgl/Sekk/k/RLSa9I+kaTPjdlN4J/SdLTki7KbXtN0suSDkrq2V+HK1jXVkm/z/Z9UNJduW19uVF9wbq+lavpkKT3JX0y29av4/UXkp6T9GJW1z816SNJ92bH5CVJm3Lb+nW8itQ1jPlVpK5hzK8idQ18fuX2XZL035J+2mTbwOdXwbp6O7+K/v3mlbxQv7/vpuzx2cCrfPSG8F8A/ip7fC3ZDeGz9deAtUOqayvw0yZjl3Wj+l7V1dD/RuDfB3C8BHwse7wGeBa4pKHPdZx674dnB3C8itQ1jPlVpK5hzK+2dQ1jfuWe/x+AB1scl4HPr4J19XR+nRav+CPifyLihezxH4Bf0nBv34h4OiJ+l60+Q/1uYEOvawlbgCMRcTQi3gMeBrYPqa6dwEO92HebuiIi3slW12RL47sPtgM/zPo+A3xC0rn093i1rWtI86vI8WplqMerwUDmF4CkddRvHPWDFl0GPr+K1NXr+XVaBH+epDHg89RfZbTyVeo/1RcFcEDSnKS+3FuuTV2XZr8W75O0MWsrfKP6PtaFpDL1G+w8kmvu2/HKft09CJwAnoiIxrpaHZe+Hq8CdeUNbH4VrGvg86vo8Rr0/KJ+U6h/BP7cYvtQ5leBuvK6nl8jnde3ckn6GPUJ9M2IeLtFnyupH7jLc82XRcRxSZ8GnpD0q4h4ckB1vUD9b2y8o/rdyn4MXEAHN6rvU12LbgR+HhFv5dr6drwi4n3gYkmfAPZK+lxEHMqX3WzYEu09UaCuenEDnl8F6hrK/Cp6vBjg/JJ0A3AiIuYkbW3VrUlbX+dXwboW+/Zkfp02r/glraEeYrMR8aMWfS6k/qvU9oh4c7E9Io5nX08Ae6n/WjeQuiLi7cVfiyPiMWCNpLX0+Ub1RY5XZgcNv4b383jl9vF/wH9QfzWY1+q49PV4FahrKPOrXV3Dml/t6soZ5Py6DPg7Sa9RP1Xzt5JmGvoMY34Vqau386uTCwIrdaH+0/iHwD1L9NkAHAG+0NB+FnB27vHTwLYB1vUZPvwg3RbgWDZuWTeq71VdWb+PA28BZw3oeI0Cn8ge/yXwFHBDQ5/rOfXi23NZez+PV5G6hjG/itQ1jPnVtq5hzK+GfW+l+UXUgc+vgnX1dH6dLqd6LgO+ArycnVcE+Db1g0VE7ALuAj4FfF8SwMmo/5W7c6j/Kgr1f9wHI+LxAdb1ZWBK0kngT8COqP8rnpR0O7CfD29U/8oA6wL4EnAgIv6YG9vP43Uu8K+SStR/G/23iPippFtydT1G/Z0XR4B3gZuzbf08XkXqGsb8KlLXMOZXkbpg8POrqRUwv4rU1dP55T/ZYGaWmNPmHL+ZmRXj4DczS4yD38wsMQ5+M7PEOPjNzBLj4DczS4yD38wsMf8Psa6eSLk2oOsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test, y_test, color='black')\n",
    "\n",
    "plt.plot(X_test, y_predict_dummy_median, color='blue', linestyle = 'dashed',linewidth=2, label = 'dummy')\n",
    "plt.plot(X_test, y_predict_dummy_mean, color='red', linestyle = 'dashed',linewidth=2, label = 'dummy');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "<hr/>\n",
    "\n",
    "#### [E] Exploring *accuracy_score*, *classification_report* , and *confusion_matrix* from sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach \n",
    "<hr/>\n",
    "\n",
    "**Accuracy Score:**\n",
    "   - Import sklearn.metrics library \n",
    "   - Getting Predicted value of quality of wine using DummyRegressor (median strategy used here)\n",
    "   - Getting the accuracy score using accuracy_score command\n",
    "   \n",
    "<br/>\n",
    "\n",
    "**Classification_report:**\n",
    "   - Since we need a classification problem, I will use iris_dataset\n",
    "   - Load Iris using sklearn.datasets\n",
    "   - Predicting target variable using KNN classifier\n",
    "   - Get the classfication report using the command available in sklear.metrics library\n",
    "   \n",
    " <br/>\n",
    " \n",
    "**Confusion Matrix:**\n",
    "   - Import confusion_matrix from sklearn.metrics\n",
    "   - I have used the same classifier problem of iris_dataset \n",
    "   - I already had predicted and true values of target variable (KNN classifier used)\n",
    "   - Getting the confusion matrix using the command available in sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skm.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "df = pd.read_csv('wineQT.csv')\n",
    "\n",
    "# separating dependent and independent variables\n",
    "X = df.drop('Id',axis=1)\n",
    "X = X.drop('quality',axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "#splitting data into training and testind data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# predicting value of quality of wine\n",
    "y_predict = DummyRegressor(strategy = 'median').fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#true value of wine quality\n",
    "y_true = list(y)[:343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  121\n"
     ]
    }
   ],
   "source": [
    "# getting the accuracy score\n",
    "print(\"Accuracy Score: \",skm.accuracy_score(y_true, y_predict, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "?skm.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data of iris_dataset\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Target_names:\n",
    "    # 0 -> Setosa\n",
    "    # 1 -> Versicolor\n",
    "    # 2 -> Virginica\n",
    "print(np.unique(iris_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value:  [1 2 1 0 0 0 2 1 2 0]\n",
      "********************************************\n",
      "Actual Value:     [1 1 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split iris data in train and test data\n",
    "# A random permutation, to split the data randomly\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train)\n",
    "KNeighborsClassifier()\n",
    "y_pred = knn.predict(iris_X_test)\n",
    "y_true = iris_y_test\n",
    "\n",
    "print(\"Predicted value: \",y_pred)\n",
    "print(\"********************************************\")\n",
    "print(\"Actual Value:    \",y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00         4\n",
      "  versicolor       1.00      0.75      0.86         4\n",
      "   virginica       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.89      0.92      0.89        10\n",
      "weighted avg       0.93      0.90      0.90        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['setosa', 'versicolor', 'virginica']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Iris_dataset prediction using KNN Classifier:\n",
      "\n",
      "[[4 0 0]\n",
      " [0 3 1]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix for Iris_dataset prediction using KNN Classifier:\")\n",
    "print()\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTIONS\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1] What are the different parameters of the above functions/methods that are part of the above SKLearn modules?\n",
    "<br>\n",
    "\n",
    "**[1] Parameters in *train_test_split* :**\n",
    "\n",
    "- <u>Arrays</u> : \n",
    "    A series of identically sized indexable Lists, numpy arrays,scipy-sparse ,matrices or dataframes in pandas are all valid inputs.\n",
    "    \n",
    "- <u>test_size (float or int, default=None)</u> :\n",
    "    If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If ``train_size`` is also None, it will be set to 0.25.\n",
    "    \n",
    "- <u>train_size (float or int, default=None)</u> :\n",
    "    If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None,the value is automatically set to the complement of the test size.\n",
    "\n",
    "- <u> random_state (int or RandomState instance, default=None)</u> :\n",
    "    Controls the shuffling applied to the data before applying the split.Pass an int for reproducible output across multiple function calls.\n",
    "    \n",
    "- <u>shuffle (bool, default=True)</u> :\n",
    "    Whether or not to shuffle the data before splitting. If shuffle=False, then stratify must be None.\n",
    "\n",
    "- <u> stratify (array-like, default=None)</u>:\n",
    "    If not None, data is split in a stratified fashion, using this as the class labels.\n",
    " \n",
    "<hr/>\n",
    "\n",
    "**[2] Parameters in *make_classification* :**\n",
    "\n",
    "- <u> n_samples (int, optional (default=100)</u>:\n",
    "    The number of samples.\n",
    "\n",
    "- <u>n_features  (int, optional (default=20))</u>:\n",
    "    The total number of features. These comprise ``n_informative`` informative features, ``n_redundant`` redundant features,``n_repeated`` duplicated features and ``n_features-n_informative-n_redundant-n_repeated`` useless features drawn at random.\n",
    "\n",
    "- <u>n_informative (int, optional (default=2))</u>:\n",
    "    The number of informative features. Each class is composed of a numberof gaussian clusters each located around the vertices of a hypercube in a subspace of dimension ``n_informative``. For each cluster, informative features are drawn independently from  N(0, 1) and then randomly linearly combined within each cluster in order to addcovariance. The clusters are then placed on the vertices of the hypercube.\n",
    "    \n",
    "- <u>n_redundant (int, optional (default=2))</u>:\n",
    "    The number of redundant features. These features are generated as random linear combinations of the informative features.\n",
    "\n",
    "- <u>n_repeated  (int, optional (default=0))</u>:\n",
    "    The number of duplicated features, drawn randomly from the informative and the redundant features.\n",
    "\n",
    "- <u>n_classes (int, optional (default=2))</u>:\n",
    "    The number of classes (or labels) of the classification problem.\n",
    "\n",
    "- <u>n_clusters_per_class (int, optional (default=2))</u>:\n",
    "    The number of clusters per class.\n",
    "\n",
    "- <u>weights(array-like of shape (n_classes,) or (n_classes - 1,),(default=None))</u>:\n",
    "    The proportions of samples assigned to each class. If None, then classes are balanced. Note that if ``len(weights) == n_classes - 1``,then the last class weight is automatically inferred.More than ``n_samples`` samples may be returned if the sum of ``weights`` exceeds 1.\n",
    "    \n",
    "- <u>flip_y (float, optional (default=0.01))</u>:\n",
    "    The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y > 0 might lead to less than n_classes in y in some cases.\n",
    "\n",
    "- <u>class_sep (float, optional (default=1.0))</u>:\n",
    "    The factor multiplying the hypercube size.  Larger values spread out the clusters/classes and make the classification task easier.\n",
    "\n",
    "- <u>hypercube (boolean, optional (default=True))</u>:\n",
    "    If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.\n",
    "\n",
    "- <u>shift (float, array of shape [n_features] or None, optional (default=0.0))</u>:\n",
    "    Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep].\n",
    "\n",
    "- <u>scale (float, array of shape [n_features] or None, optional (default=1.0))</u>:\n",
    "    Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.\n",
    "\n",
    "- <u>shuffle (boolean, optional (default=True))</u>:\n",
    "    Shuffle the samples and the features.\n",
    "\n",
    "- <u>random_state (int, RandomState instance, default=None)</u>:\n",
    "    Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[3] Parameters in *load_iris* :**\n",
    "\n",
    "- <u>return_X_y (bool, default=False)</u>:\n",
    "    If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.\n",
    "\n",
    "- <u>as_frame (bool, default=False)</u>:\n",
    "    If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric). The target is a pandas DataFrame or Series depending on the number of target columns. If `return_X_y` is True, then (`data`, `target`) will be pandas DataFrames or Series as described below.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[4] Parameters in *make_regression* :**\n",
    "\n",
    "- <u>n_samples (int, optional (default=100))</u>:\n",
    "    The number of samples.\n",
    "\n",
    "- <u>n_features (int, optional (default=100))</u>:\n",
    "    The number of features.\n",
    "\n",
    "- <u>n_informative (int, optional (default=10))</u>:\n",
    "    The number of informative features, i.e., the number of features used to build the linear model used to generate the output.\n",
    "\n",
    "- <u>n_targets (int, optional (default=1))</u>:\n",
    "    The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.\n",
    "\n",
    "- <u>bias (float, optional (default=0.0))</u>:\n",
    "    The bias term in the underlying linear model.\n",
    "\n",
    "- <u>effective_rank (int or None, optional (default=None))</u>:\n",
    "    - if not None:\n",
    "        The approximate number of singular vectors required to explain mostof the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.\n",
    "    - if None:\n",
    "        The input set is well conditioned, centered and gaussian with unit variance.\n",
    "        \n",
    "- <u>noise (float, optional (default=0.0))</u>:\n",
    "    The standard deviation of the gaussian noise applied to the output.\n",
    "\n",
    "- <u>shuffle (boolean, optional (default=True))</u>:\n",
    "    Shuffle the samples and the features.\n",
    "\n",
    "- <u>coef (boolean, optional (default=False))</u>:\n",
    "    If True, the coefficients of the underlying linear model are returned.\n",
    "\n",
    "- <u>random_state (int, RandomState instance, default=None)</u>:\n",
    "    Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[5] Parameters in *load_boston* :**\n",
    "\n",
    "- <u>return_X_y (bool, default=False)</u>:.\n",
    "    If True, returns ``(data, target)`` instead of a Bunch object.See below for more information about the `data` and `target` object.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[6] Parameters in *DummyClassifier* :**\n",
    "\n",
    "- <u>strategy (str, default=\"stratified\")</u>:\n",
    "    Strategy to use to generate predictions:\n",
    "    * \"stratified\": generates predictions by respecting the trainingset's class distribution.\n",
    "    * \"most_frequent\": always predicts the most frequent label in the training set.\n",
    "    * \"prior\": always predicts the class that maximizes the class prior (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n",
    "    * \"uniform\": generates predictions uniformly at random.\n",
    "    * \"constant\": always predicts a constant label that is provided bythe user. This is useful for metrics that evaluate a non-majority class.\n",
    "\n",
    "- <u>random_state (int, RandomState instance or None, optional, default=None)</u>:\n",
    "    Controls the randomness to generate the predictions when``strategy='stratified'`` or ``strategy='uniform'``. Pass an int for reproducible output across multiple function calls.\n",
    " \n",
    "- <u>constant (int or str or array-like of shape (n_outputs,)</u>:\n",
    "    The explicit constant as predicted by the \"constant\" strategy. This parameter is useful only for the \"constant\" strategy.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[7] Parameters in *DummyRegressor* :**\n",
    "\n",
    "- <u>strategy (str)</u>:\n",
    "    Strategy to use to generate predictions:\n",
    "    * \"mean\": always predicts the mean of the training set\n",
    "    * \"median\": always predicts the median of the training set\n",
    "    * \"quantile\": always predicts a specified quantile of the training set, provided with the quantile parameter.\n",
    "    * \"constant\": always predicts a constant value that is provided by the user.\n",
    "\n",
    "- <u>constant (int or float or array-like of shape (n_outputs,))</u>:\n",
    "    The explicit constant as predicted by the \"constant\" strategy. This parameter is useful only for the \"constant\" strategy.\n",
    "\n",
    "- <u>quantile (float in [0.0, 1.0])</u>:\n",
    "    The quantile to predict using the \"quantile\" strategy. A quantile of0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.\n",
    "  \n",
    "<hr/>\n",
    "\n",
    "**[8] Parameters in *accuracy_score* :**  \n",
    "- <u>y_true (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Ground truth (correct) labels.\n",
    "\n",
    "- <u>y_pred (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Predicted labels, as returned by a classifier.\n",
    "\n",
    "- <u>normalize (bool, optional (default=True))</u>:\n",
    "    If ``False``, return the number of correctly classified samples.Otherwise, return the fraction of correctly classified samples.\n",
    "\n",
    "- <u>sample_weight (array-like of shape (n_samples,), default=None)</u>:\n",
    "    Sample weights.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**[9] Parameters in *classification_report* :**    \n",
    "    \n",
    "- <u>y_true (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "- <u>y_pred (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Estimated targets as returned by a classifier.\n",
    "\n",
    "- <u>labels (array, shape = [n_labels])</u>:\n",
    "    Optional list of label indices to include in the report.\n",
    "\n",
    "- <u>target_names (list of strings)</u>:\n",
    "    Optional display names matching the labels (same order).\n",
    "\n",
    "- <u>sample_weight (array-like of shape (n_samples,), default=None)</u>:\n",
    "    Sample weights.\n",
    "\n",
    "- <u>digits (int)</u>:\n",
    "    Number of digits for formatting output floating point values.When ``output_dict`` is ``True``, this will be ignored and the returned values will not be rounded.\n",
    "\n",
    "- <u>output_dict (bool (default = False))</u>:\n",
    "    If True, return output as dict\n",
    "\n",
    "- <u>zero_division (\"warn\", 0 or 1, default=\"warn\")</u>:\n",
    "    Sets the value to return when there is a zero division. If set to \"warn\", this acts as 0, but warnings are also raised.\n",
    "    \n",
    "<hr/>\n",
    "\n",
    "**[10] Parameters in *confusion_matrix* :**  \n",
    "\n",
    "- <u>y_true (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Ground truth (correct) target values.\n",
    "\n",
    "- <u>y_pred (1d array-like, or label indicator array / sparse matrix)</u>:\n",
    "    Estimated targets as returned by a classifier.\n",
    "\n",
    "- <u>labels (array-like of shape (n_classes), default=None)</u>:\n",
    "    List of labels to index the matrix. This may be used to reorderor select a subset of labels.If ``None`` is given, those that appear at least once in ``y_true`` or ``y_pred`` are used in sorted order.\n",
    "\n",
    "- <u>sample_weight (array-like of shape (n_samples,), default=None)</u>:\n",
    "    Sample weights.\n",
    "    \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] What is the effect, when you modify certain parameters that are present in the same?\n",
    "\n",
    "<br>\n",
    "\n",
    "Modifying parameters will affect the output. So, we need to understand the working of each parameter and use it wisely according to our problem statement.\n",
    "\n",
    "**Example:**\n",
    "![DummyClassifier](example.png)\n",
    "\n",
    "- Here, we can see how test_score varies for different parameters (DummyClassifier)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] How to get different train and test datasets?\n",
    "\n",
    "We can get different train and test datasets by splitting the main datasets into 2 parts. We can achieve this by using *train_test_split* command of sklearn's model_selection sub-library. We can also specify the ratio of split using the parameters of the command.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Identify which are features and which are targets in Part 1b, and Part 1c (make_classification and make_regression would depend on your inputs on the function call)\n",
    "\n",
    "<br>\n",
    "\n",
    "**In part 1B:**\n",
    "- <u>Features</u>:\n",
    "   - sepal length (cm)\n",
    "   - sepal width (cm)\n",
    "   - petal length (cm)\n",
    "   - petal width (cm)\n",
    "   \n",
    "<br>\n",
    "\n",
    "- <u>Target</u>:\n",
    "   - setosa\n",
    "   - versicolor \n",
    "   - virginica\n",
    " \n",
    "<hr>\n",
    "\n",
    "**In 1c:**\n",
    "- <u>Features</u>:\n",
    "   - CRIM\n",
    "   - ZN\n",
    "   - INDUS\n",
    "   - CHAS\n",
    "   - NOX\n",
    "   - RM \n",
    "   - AGE\n",
    "   - DIS  \n",
    "   - RAD\n",
    "   - TAX \n",
    "   - PTRATIOB\n",
    "   - LSTAT\n",
    "   \n",
    "<br>\n",
    "\n",
    "- <u>Target</u>:\n",
    "   - Median value of owner-occupied homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5] Identify what the things mentioned in Part 1d stands for. \n",
    "\n",
    "<hr>\n",
    "\n",
    "**DummyClassifier:** \n",
    "It is a classifier that makes predictions using simple rules.This classifier is useful as a simple baseline to compare with other (real) classifiers. It is not advised to be used on real problems. For real problems we can use something like KKN classifier etc.,\n",
    "\n",
    "<br>\n",
    "\n",
    "**DummyRegressor:** \n",
    "It is a regressor that makes predictions using\n",
    "simple rules.This regressor is useful as a simple baseline to compare with other\n",
    "(real) regressors. Do not use it for real problems.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [6] Making use of Part 1d, explore the various options available under Part 1e.\n",
    "\n",
    "**Accuracy_score:**\n",
    "In multilabel classification, this function computes subset accuracythe set of labels predicted for a sample must *exactly* match the corresponding set of labels in y_true.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Classification Report:**\n",
    "It Builds a text report showing the main classification metrics.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Confusion Matrix:**\n",
    "We Compute confusion matrix to evaluate the accuracy of a classification.By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}` is equal to the number of observations known to be in group :math:`i` and predicted to be in group :math:`j`.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<hr>\n",
    "\n",
    "- In this lab, I learnt about various toy datasets and how useful they are in making us learn and explore new libraries, so that we can efficiently use the parameters of various commands when it comes to real-life datasets in future. \n",
    "\n",
    "- It was quite tedious to do all these and documenting it in jupyter was stressful. Yet I am happy that I learnt few new things about sklearn and will be more confident while working on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
